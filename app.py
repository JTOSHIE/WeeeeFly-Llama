import requests

# Configuration settings
API_URL = "https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct"
API_KEY = "your_huggingface_api_key"  # Replace this with the appropriate method to access your API key securely

# Set up the headers for the API request, including authorization
headers = {"Authorization": f"Bearer {API_KEY}"}

def query(payload):
    """
    Function to send a query to the Hugging Face API.

    Args:
    - payload (dict): A dictionary containing the input data for the model.

    Returns:
    - dict: A dictionary containing the response from the API.
    """
    response = requests.post(API_URL, headers=headers, json=payload)
    if response.status_code != 200:
        raise Exception(f"Error querying the Hugging Face API: {response.status_code} {response.text}")
    return response.json()

def generate_response(user_input):
    """
    Function to generate a response using the LLaMA model on Hugging Face.

    Args:
    - user_input (str): The user's input message.

    Returns:
    - str: The response generated by the AI model.
    """
    payload = {"inputs": user_input}
    try:
        output = query(payload)
        if isinstance(output, list) and len(output) > 0:
            response = output[0].get('generated_text', 'No response generated')
        else:
            response = str(output)  # Fallback if the response is not in the expected format
    except Exception as e:
        response = f"An error occurred: {str(e)}"
    return response

def main():
    """
    Main function to run the AI chatbot in a command-line interface (CLI).
    """
    print("WeeeFly AI Chatbot CLI")
    print("Type 'exit' to quit the chatbot.")
    
    while True:
        user_input = input("Enter your message: ")
        if user_input.lower() == 'exit':
            print("Exiting WeeeFly AI Chatbot. Goodbye!")
            break

        response = generate_response(user_input)
        print("AI Response:", response)

if __name__ == "__main__":
    main()
